import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from duckduckgo_search import DDGS
import re
import json
import os
from datetime import datetime

# Thi·∫øt l·∫≠p trang
st.set_page_config(page_title="Chatbot T∆∞ v·∫•n Thi·∫øt b·ªã Y t·∫ø", page_icon="üè•")

# ƒê∆∞·ªùng d·∫´n file l∆∞u l·ªãch s·ª≠
HISTORY_FILE = "medical_equipment_chat_history.json"

# Kh·ªüi t·∫°o l·ªãch s·ª≠ h·ªôi tho·∫°i
if "history" not in st.session_state:
    st.session_state.history = []
    
    # T·∫£i l·ªãch s·ª≠ t·ª´ file n·∫øu t·ªìn t·∫°i
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                st.session_state.history = json.load(f)
        except Exception as e:
            st.error(f"L·ªói khi t·∫£i l·ªãch s·ª≠: {e}")
            st.session_state.history = []

# H√†m l∆∞u l·ªãch s·ª≠ v√†o file
def save_chat_history():
    try:
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(st.session_state.history, f, ensure_ascii=False, indent=2)
    except Exception as e:
        st.error(f"L·ªói khi l∆∞u l·ªãch s·ª≠: {e}")

# T·∫£i models v·ªõi caching
@st.cache_resource
def load_models():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # T·∫£i model chuy√™n v·ªÅ thi·∫øt b·ªã y t·∫ø
    medical_tokenizer = AutoTokenizer.from_pretrained('./models/mechanic_model')
    medical_model = AutoModelForCausalLM.from_pretrained(
        './models/mechanic_model'
    ).to(device)
    
    return medical_tokenizer, medical_model, device

# H√†m t√¨m ki·∫øm DuckDuckGo
def search_duckduckgo(query, max_results=3):
    try:
        with DDGS() as ddgs:
            results = list(ddgs.text(query, region='vn-vn', max_results=max_results))
            return results
    except Exception as e:
        st.error(f"L·ªói khi t√¨m ki·∫øm: {e}")
        return []

# H√†m x√°c ƒë·ªãnh xem c√≥ c·∫ßn t√¨m ki·∫øm kh√¥ng
def needs_search(query):
    # C√°c t·ª´ kh√≥a cho th·∫•y c·∫ßn t√¨m ki·∫øm th√¥ng tin m·ªõi
    search_keywords = [
        'm·ªõi nh·∫•t', 'hi·ªán nay', 'g·∫ßn ƒë√¢y', 'c·∫≠p nh·∫≠t', 
        'th√¥ng tin', 't√¨m ki·∫øm', 'tra c·ª©u', 'h∆∞·ªõng d·∫´n m·ªõi'
    ]
    
    # Ki·ªÉm tra n·∫øu c√¢u h·ªèi ch·ª©a t·ª´ kh√≥a t√¨m ki·∫øm
    return any(keyword in query.lower() for keyword in search_keywords)

# H√†m t·∫°o b·ªëi c·∫£nh t·ª´ l·ªãch s·ª≠
def generate_conversation_context(history):
    if not history:
        return "Ch∆∞a c√≥ l·ªãch s·ª≠ tr√≤ chuy·ªán tr∆∞·ªõc ƒë√≥."
    
    context = "L·ªãch s·ª≠ tr√≤ chuy·ªán tr∆∞·ªõc ƒë√¢y:\n"
    for i, (user_msg, bot_msg) in enumerate(history[-5:]):  # L·∫•y 5 tin nh·∫Øn g·∫ßn nh·∫•t
        context += f"Ng∆∞·ªùi d√πng: {user_msg}\n"
        context += f"Chuy√™n gia: {bot_msg}\n"
    
    return context

# H√†m t·∫°o ph·∫£n h·ªìi v·ªõi l·ªãch s·ª≠ h·ªôi tho·∫°i v√† t√¨m ki·∫øm
def generate_response(user_input, tokenizer, model, history, device):
    # Ki·ªÉm tra xem c√≥ c·∫ßn t√¨m ki·∫øm th√¥ng tin kh√¥ng
    search_results = []
    if needs_search(user_input):
        with st.spinner("ƒêang t√¨m ki·∫øm th√¥ng tin m·ªõi nh·∫•t..."):
            search_query = f"thi·∫øt b·ªã y t·∫ø {user_input} s·ª≠a ch·ªØa b·∫£o tr√¨"
            search_results = search_duckduckgo(search_query)
    
    # T·∫°o b·ªëi c·∫£nh t·ª´ l·ªãch s·ª≠
    conversation_context = generate_conversation_context(history)
    
    # Th√™m k·∫øt qu·∫£ t√¨m ki·∫øm v√†o prompt n·∫øu c√≥
    search_context = ""
    if search_results:
        search_context = "Th√¥ng tin t√¨m ki·∫øm ƒë∆∞·ª£c t·ª´ internet:\n"
        for i, result in enumerate(search_results):
            search_context += f"{i+1}. {result['title']}: {result['body']}\n"
        search_context += "\n"
    
    system_prompt = f"""B·∫°n l√† chuy√™n gia k·ªπ thu·∫≠t v·ªÅ s·ª≠a ch·ªØa, b·∫£o tr√¨ v√† ki·ªÉm tra thi·∫øt b·ªã y t·∫ø. 
H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n ki·∫øn th·ª©c chuy√™n m√¥n v·ªÅ:
- ƒêo l∆∞·ªùng v√† ph√¢n t√≠ch c√°c th√¥ng s·ªë ƒëi·ªán (ƒëi·ªán √°p, d√≤ng ƒëi·ªán, c√¥ng su·∫•t)
- Chu·∫©n ƒëo√°n s·ª± c·ªë thi·∫øt b·ªã y t·∫ø
- Quy tr√¨nh b·∫£o tr√¨ ph√≤ng ng·ª´a
- An to√†n ƒëi·ªán trong thi·∫øt b·ªã y t·∫ø

S·ª≠ d·ª•ng th√¥ng tin t·ª´ l·ªãch s·ª≠ tr√≤ chuy·ªán tr∆∞·ªõc ƒë√¢y ƒë·ªÉ cung c·∫•p c√¢u tr·∫£ l·ªùi ph√π h·ª£p v√† c√° nh√¢n h√≥a.

{conversation_context}

{search_context}

C√¢u h·ªèi: {user_input}

Tr·∫£ l·ªùi:"""
    
    inputs = tokenizer(system_prompt, return_tensors="pt", truncation=True, max_length=1024).to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=1024,
            num_beams=5,
            early_stopping=True,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Lo·∫°i b·ªè ph·∫ßn prompt ƒë√£ c√≥ trong c√¢u tr·∫£ l·ªùi
    response = response.replace(system_prompt, "").strip()
    
    # Th√™m ngu·ªìn tham kh·∫£o n·∫øu c√≥ k·∫øt qu·∫£ t√¨m ki·∫øm
    if search_results:
        response += "\n\n---\n*Th√¥ng tin ƒë∆∞·ª£c tham kh·∫£o t·ª´ c√°c ngu·ªìn t√¨m ki·∫øm tr·ª±c tuy·∫øn*"
    
    return response

# Giao di·ªán ch√≠nh
st.title("üè• Chatbot T∆∞ v·∫•n K·ªπ thu·∫≠t Thi·∫øt b·ªã Y t·∫ø")
st.write("Chatbot h·ªó tr·ª£ t∆∞ v·∫•n s·ª≠a ch·ªØa, b·∫£o tr√¨ v√† ki·ªÉm tra thi·∫øt b·ªã y t·∫ø d·ª±a tr√™n c√°c th√¥ng s·ªë k·ªπ thu·∫≠t")

# T·∫£i models
medical_tokenizer, medical_model, device = load_models()
models_loaded = True

# Hi·ªÉn th·ªã l·ªãch s·ª≠ h·ªôi tho·∫°i
for user_msg, bot_msg in st.session_state.history:
    with st.chat_message("user"):
        st.write(user_msg)
    with st.chat_message("assistant"):
        st.write(bot_msg)

# Nh·∫≠n input t·ª´ ng∆∞·ªùi d√πng
user_input = st.chat_input("Nh·∫≠p v·∫•n ƒë·ªÅ v·ªÅ thi·∫øt b·ªã y t·∫ø...")

if user_input and models_loaded:
    # Hi·ªÉn th·ªã c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
    with st.chat_message("user"):
        st.write(user_input)

    # T·∫°o ph·∫£n h·ªìi b·∫±ng model
    with st.spinner("ƒêang ph√¢n t√≠ch v·∫•n ƒë·ªÅ..."):
        history_messages = [(user, bot) for user, bot in st.session_state.history]
        response = generate_response(user_input, medical_tokenizer, medical_model, history_messages, device)
    
    # Hi·ªÉn th·ªã ph·∫£n h·ªìi
    with st.chat_message("assistant"):
        st.write(response)
    
    # L∆∞u v√†o l·ªãch s·ª≠
    st.session_state.history.append((user_input, response))
    
    # T·ª± ƒë·ªông l∆∞u l·ªãch s·ª≠ v√†o file
    save_chat_history()
else:
    response = ""

# N√∫t x√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i
if st.button("X√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i"):
    st.session_state.history = []
    # X√≥a file l·ªãch s·ª≠
    if os.path.exists(HISTORY_FILE):
        os.remove(HISTORY_FILE)
    st.rerun()

# Hi·ªÉn th·ªã th√¥ng tin debug
with st.expander("Th√¥ng tin k·ªπ thu·∫≠t (cho chuy√™n gia)"):
    if user_input and models_loaded:
        st.write("**C√¢u h·ªèi:**", user_input)
        st.write("**Ph·∫£n h·ªìi:**", response)
    
    # Hi·ªÉn th·ªã t·ªïng s·ªë tin nh·∫Øn trong l·ªãch s·ª≠
    st.write(f"**T·ªïng s·ªë tin nh·∫Øn trong l·ªãch s·ª≠:** {len(st.session_state.history)}")
    
    # N√∫t xu·∫•t l·ªãch s·ª≠
    if st.button("Xu·∫•t l·ªãch s·ª≠ tr√≤ chuy·ªán"):
        with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
            st.download_button(
                label="T·∫£i xu·ªëng l·ªãch s·ª≠",
                data=f,
                file_name="history_backup.json",
                mime="application/json"
            )
import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel
import torch
from duckduckgo_search import DDGS
import json
from dotenv import load_dotenv
import os
import time

# Load bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
load_dotenv()
hf_token = os.getenv("HF_TOKEN")
model_name = "google/gemma-3-1b-it"

# Thi·∫øt l·∫≠p trang
st.set_page_config(page_title="Chatbot T∆∞ v·∫•n Thi·∫øt b·ªã Y t·∫ø", page_icon="üè•")

# ƒê∆∞·ªùng d·∫´n file l∆∞u l·ªãch s·ª≠
HISTORY_FILE = "medical_equipment_chat_history.json"

# Kh·ªüi t·∫°o l·ªãch s·ª≠ h·ªôi tho·∫°i
if "history" not in st.session_state:
    st.session_state.history = []
    
    # T·∫£i l·ªãch s·ª≠ t·ª´ file n·∫øu t·ªìn t·∫°i
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                st.session_state.history = json.load(f)
        except Exception as e:
            st.error(f"L·ªói khi t·∫£i l·ªãch s·ª≠: {e}")
            st.session_state.history = []

# H√†m l∆∞u l·ªãch s·ª≠ v√†o file
def save_chat_history():
    try:
        with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(st.session_state.history, f, ensure_ascii=False, indent=2)
    except Exception as e:
        st.error(f"L·ªói khi l∆∞u l·ªãch s·ª≠: {e}")

# T·∫£i models v·ªõi caching v√† qu·∫£n l√Ω b·ªô nh·ªõ
@st.cache_resource
def load_models():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    st.info(f"ƒêang s·ª≠ d·ª•ng thi·∫øt b·ªã: {device}")

    try:
        # T·∫£i tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            model_name, 
            token=hf_token
        )
        
        # Ki·ªÉm tra b·ªô nh·ªõ GPU kh·∫£ d·ª•ng
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            st.info(f"GPU Memory: {gpu_memory:.1f} GB")
            
            # S·ª≠ d·ª•ng c√°c t√πy ch·ªçn ti·∫øt ki·ªám b·ªô nh·ªõ
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                token=hf_token,
                torch_dtype="auto",
                device_map="auto",
            )
        else:
            # Tr√™n CPU, s·ª≠ d·ª•ng float32 th√¥ng th∆∞·ªùng
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                token=hf_token,
                torch_dtype=torch.float32,
                device_map={"": device},
            )
        
        # ƒê·∫£m b·∫£o tokenizer c√≥ pad token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        return tokenizer, model, device
    except Exception as e:
        st.error(f"L·ªói khi t·∫£i model: {e}")
        return None, None, None
# H√†m t√¨m ki·∫øm DuckDuckGo
def search_duckduckgo(query, max_results=3):
    try:
        with DDGS() as ddgs:
            results = list(ddgs.text(query, region='vn-vn', max_results=max_results))
            return results
    except Exception as e:
        st.error(f"L·ªói khi t√¨m ki·∫øm: {e}")
        return []

# H√†m t·∫°o b·ªëi c·∫£nh t·ª´ l·ªãch s·ª≠
def generate_conversation_context(history):
    if not history:
        return ""
    
    context = "L·ªãch s·ª≠ tr√≤ chuy·ªán tr∆∞·ªõc ƒë√¢y:\n"
    for i, (user_msg, bot_msg) in enumerate(history[-3:]):
        context += f"Ng∆∞·ªùi d√πng: {user_msg}\n"
        context += f"Chuy√™n gia: {bot_msg}\n"
    
    return context

def generate_search_context(user_input, tokenizer, model, device):
    prompt = f"T·ª´ c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:{user_input}, h√£y t·∫°o m·ªôt truy v·∫•n t√¨m ki·∫øm th√¥ng tin li√™n quan ƒë·ªÉ tra c·ª©u tr√™n internet. Truy v·∫•n n√™n ng·∫Øn g·ªçn v√† t·∫≠p trung v√†o c√°c t·ª´ kh√≥a ch√≠nh."
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=64,
            num_beams=3,
            early_stopping=True,
            temperature=1.0,
            top_k = 64,
            top_p = 0.95,
            min_p = 0.0,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    search_query = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()
    return search_query

# H√†m t·∫°o prompt hi·ªáu qu·∫£
def create_efficient_prompt(user_input, conversation_context):
    return f"""B·∫°n l√† chuy√™n gia k·ªπ thu·∫≠t v·ªÅ s·ª≠a ch·ªØa, b·∫£o tr√¨ v√† ki·ªÉm tra thi·∫øt b·ªã y t·∫ø.

{conversation_context}

C√¢u h·ªèi: {user_input}

Tr·∫£ l·ªùi ng·∫Øn g·ªçn, t·∫≠p trung v√†o v·∫•n ƒë·ªÅ:"""

# H√†m t·∫°o ph·∫£n h·ªìi v·ªõi model
def generate_response(user_input, tokenizer, model, history, device):
    # T√¨m ki·∫øm th√¥ng tin
    # search_query = generate_search_context(user_input, tokenizer, model, device)
    # search_results = search_duckduckgo(search_query)
    
    # T·∫°o b·ªëi c·∫£nh t·ª´ l·ªãch s·ª≠
    conversation_context = generate_conversation_context(history)
    
    # # Th√™m k·∫øt qu·∫£ t√¨m ki·∫øm v√†o prompt
    # search_context = ""
    # if search_results:
    #     search_context = "TH√îNG TIN T√åM KI·∫æM ƒê∆Ø·ª¢C:\n"
    #     for i, result in enumerate(search_results[:2]):  # Gi·ªõi h·∫°n s·ªë k·∫øt qu·∫£
    #         search_context += f"{i+1}. {result['title']}: {result['body'][:150]}...\n"
    # else:
    #     search_context = "Kh√¥ng t√¨m th·∫•y th√¥ng tin m·ªõi t·ª´ internet.\n"
    
    # T·∫°o prompt hi·ªáu qu·∫£
    prompt = create_efficient_prompt(user_input, conversation_context)
    
    # Tokenize v√† generate
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024).to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,  # Gi·∫£m ƒë·ªô d√†i ph·∫£n h·ªìi
            num_beams=3,  # Gi·∫£m s·ªë beams ƒë·ªÉ tƒÉng t·ªëc
            early_stopping=True,
            temperature=1.0,
            top_k = 64,
            top_p = 0.95,
            min_p = 0.0,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # Decode ch·ªâ ph·∫ßn ph·∫£n h·ªìi ƒë∆∞·ª£c t·∫°o (b·ªè qua prompt)
    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    
    # ƒê·∫£m b·∫£o ph·∫£n h·ªìi kh√¥ng r·ªóng
    if not response.strip():
        response = "T√¥i hi·ªÉu c√¢u h·ªèi c·ªßa b·∫°n. D·ª±a tr√™n ki·∫øn th·ª©c chuy√™n m√¥n, "
        response += "t√¥i khuy√™n b·∫°n n√™n ki·ªÉm tra c√°c th√¥ng s·ªë ƒëi·ªán c∆° b·∫£n v√† li√™n h·ªá v·ªõi k·ªπ thu·∫≠t vi√™n c√≥ chuy√™n m√¥n."
    
    # # Th√™m ngu·ªìn tham kh·∫£o n·∫øu c√≥ k·∫øt qu·∫£ t√¨m ki·∫øm
    # if search_results:
    #     response += "\n\n---\n*Th√¥ng tin tham kh·∫£o t·ª´ c√°c ngu·ªìn tr·ª±c tuy·∫øn*"
    
    return response

# H√†m hi·ªÉn th·ªã response t·ª´ng ph·∫ßn (streaming)
def stream_response(response):
    placeholder = st.empty()
    full_response = ""
    for chunk in response.split():
        full_response += chunk + " "
        placeholder.markdown(full_response + "‚ñå")
        time.sleep(0.03)  # Gi·∫£ l·∫≠p t·ªëc ƒë·ªô g√µ
    placeholder.markdown(full_response)
    return full_response

# Giao di·ªán ch√≠nh
st.title("üè• Chatbot T∆∞ v·∫•n K·ªπ thu·∫≠t Thi·∫øt b·ªã Y t·∫ø")
st.write("Chatbot h·ªó tr·ª£ t∆∞ v·∫•n s·ª≠a ch·ªØa, b·∫£o tr√¨ v√† ki·ªÉm tra thi·∫øt b·ªã y t·∫ø")

# Hi·ªÉn th·ªã c·∫£nh b√°o v·ªÅ hi·ªáu su·∫•t
if torch.cuda.is_available():
    st.success("ƒê√£ ph√°t hi·ªán GPU, s·ª≠ d·ª•ng ch·∫ø ƒë·ªô tƒÉng t·ªëc")
else:
    st.warning("Kh√¥ng ph√°t hi·ªán GPU, chatbot s·∫Ω ch·∫°y tr√™n CPU (c√≥ th·ªÉ ch·∫≠m h∆°n)")

# T·∫£i models
with st.spinner("ƒêang t·∫£i model, vui l√≤ng ch·ªù..."):
    medical_tokenizer, medical_model, device = load_models()
models_loaded = medical_model is not None

if not models_loaded:
    st.error("Kh√¥ng th·ªÉ t·∫£i model. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi v√† th·ª≠ l·∫°i.")
    st.stop()

# Hi·ªÉn th·ªã l·ªãch s·ª≠ h·ªôi tho·∫°i
for user_msg, bot_msg in st.session_state.history:
    with st.chat_message("user"):
        st.write(user_msg)
    with st.chat_message("assistant"):
        st.write(bot_msg)

# Nh·∫≠n input t·ª´ ng∆∞·ªùi d√πng
user_input = st.chat_input("Nh·∫≠p v·∫•n ƒë·ªÅ v·ªÅ thi·∫øt b·ªã y t·∫ø...")

if user_input and models_loaded:
    # Hi·ªÉn th·ªã c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
    with st.chat_message("user"):
        st.write(user_input)

    # T·∫°o ph·∫£n h·ªìi b·∫±ng model
    with st.spinner("ƒêang ph√¢n t√≠ch v√† t√¨m ki·∫øm th√¥ng tin..."):
        history_messages = [(user, bot) for user, bot in st.session_state.history]
        response = generate_response(user_input, medical_tokenizer, medical_model, history_messages, device)
    
    # Hi·ªÉn th·ªã ph·∫£n h·ªìi v·ªõi hi·ªáu ·ª©ng streaming
    with st.chat_message("assistant"):
        full_response = stream_response(response)
    
    # L∆∞u v√†o l·ªãch s·ª≠
    st.session_state.history.append((user_input, full_response))
    
    # T·ª± ƒë·ªông l∆∞u l·ªãch s·ª≠ v√†o file
    save_chat_history()

# N√∫t x√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i
if st.button("X√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i"):
    st.session_state.history = []
    # X√≥a file l·ªãch s·ª≠
    if os.path.exists(HISTORY_FILE):
        os.remove(HISTORY_FILE)
    st.rerun()

# Hi·ªÉn th·ªã th√¥ng tin k·ªπ thu·∫≠t
with st.expander("Th√¥ng tin k·ªπ thu·∫≠t (cho chuy√™n gia)"):
    if user_input and models_loaded:
        st.write("**C√¢u h·ªèi:**", user_input)
    
    # Hi·ªÉn th·ªã t·ªïng s·ªë tin nh·∫Øn trong l·ªãch s·ª≠
    st.write(f"**T·ªïng s·ªë tin nh·∫Øn trong l·ªãch s·ª≠:** {len(st.session_state.history)}")
    
    # Hi·ªÉn th·ªã th√¥ng tin device
    st.write(f"**Thi·∫øt b·ªã ƒëang s·ª≠ d·ª•ng:** {device}")
    
    # N√∫t xu·∫•t l·ªãch s·ª≠
    if st.button("Xu·∫•t l·ªãch s·ª≠ tr√≤ chuy·ªán"):
        with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
            st.download_button(
                label="T·∫£i xu·ªëng l·ªãch s·ª≠",
                data=f,
                file_name="history_backup.json",
                mime="application/json"
            )